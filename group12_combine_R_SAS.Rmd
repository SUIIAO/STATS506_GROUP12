---
title: "Multinomial Logistic Regression in R, Stata and SAS"
author: "Yunsun Lee, Hui Xu, Su I Iao (Group 12)"
date: "November 26, 2018"
output:
  html_document: default
  pdf_document: default
---


## Algorithm Description
The purpose of this tutorial is to demonstrate multinomial logistic regression in R, Stata and SAS. The following is a brief summary of the [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression).

One fairly simple way to arrive at the multinomial logit model is to imagine, for K possible outcomes, running K-1 independent binary logistic regression models, in which one outcome is chosen as a "pivot" and then the other K-1 outcomes are separately regressed against the pivot outcome. This would proceed as follows, if outcome K (the last outcome) is chosen as the pivot:
$$ln\frac{Pr(Y_{i} = 1)}{Pr(Y_{i} = K)} = \beta_{1}\cdot X_{i}$$
$$ln\frac{Pr(Y_{i} = 2)}{Pr(Y_{i} = K)} = \beta_{2}\cdot X_{i}$$
$$\cdots$$
$$ln\frac{Pr(Y_{i} = K-1)}{Pr(Y_{i} = K)} = \beta_{K-1}\cdot X_{i}$$
Note that we have introduced separate sets of regression coefficients, one for each possible outcome.
$$Pr(Y_{i} = 1) = Pr(Y_{i} = K) \cdot e^{\beta_{1}\cdot X_{i}}$$
$$Pr(Y_{i} = 2) = Pr(Y_{i} = K) \cdot e^{\beta_{2}\cdot X_{i}}$$
$$\cdots$$
$$Pr(Y_{i} = K-1) = Pr(Y_{i} = K) \cdot e^{\beta_{K-1}\cdot X_{i}}$$
Using the fact that all K of the probabilities must sum to one, we find:
$$Pr(Y_{i} = K) = 1 - \sum^{K-1}_{k=1}Pr(Y_{i} = k)\cdot e^{\beta_{k}\cdot X_{i}}$$
$$Pr(Y_{i} = K) = \frac{1}{1 +  \sum^{K-1}_{k=1}e^{\beta_{k}\cdot X_{i}}}$$
We can use this to find the other probabilities:
$$Pr(Y_{i} = 1) = \frac{e^{\beta_{1}\cdot X_{i}}}{1 +  \sum^{K-1}_{k=1}e^{\beta_{k}\cdot X_{i}}}$$
$$Pr(Y_{i} = 2) = \frac{e^{\beta_{2}\cdot X_{i}}}{1 +  \sum^{K-1}_{k=1}e^{\beta_{k}\cdot X_{i}}}$$
$$\cdots$$
$$Pr(Y_{i} = K-1) = \frac{e^{\beta_{K-1}\cdot X_{i}}}{1 +  \sum^{K-1}_{k=1}e^{\beta_{k}\cdot X_{i}}}$$
$$Pr(Y_{i} = K) = \frac{1}{1 +  \sum^{K-1}_{k=1}e^{\beta_{k}\cdot X_{i}}}$$
The fact that we run multiple regressions reveals why the model relies on the assumption of independence of irrelevant alternatives described above.

## Data Summary
In this tutorial, we will work on the *Iris flower data set*. This famout Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper *"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis."* It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. 

The data set consists of 50 samples from each of three species of Iris(Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample : the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other. Based on Fisher's linear discriminant model, this data set became a typical test case for many statistical classification techniques. 

Below are pictures of three species of Iris flowers. 

![](https://github.com/SUIIAO/STATS506_GROUP12/raw/master/iris_picture.png)

We can obtain Iris data from [here](https://www.kaggle.com/vvenanccio/irisflowers). Also, this dataset is already contained in R. Therefore just by loading it, Iris data is available to use in R. The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species. Description of variables are as followed.

| Variables | Description | Data type |
|----------------|-------------------------------------|-------------------------|
| Sepal.Length   | sepal length in centimeters         |positive real number |
| Sepal.Width    | sepal width in centimeters          |positive real number |
| Petal.Length   | petal length in centimeters         |positive real number |
| petal.Width    | petal width in centimeters          |positive real number |
| Species        | species                             |categorical(setosa/versicolour/virginica) |


## Our Model
$$ln\frac{Pr(Species = setosa)}{Pr(Species = virginica)} = \beta_{10} + \beta_{11}\cdot Sepal.Length + \beta_{12}\cdot Sepal.Width + \beta_{13}\cdot Petal.Length + \beta_{14}\cdot Petal.Width X_{i}$$
$$ln\frac{Pr(Species = versicolor)}{Pr(Species = virginica)} = \beta_{20} + \beta_{21}\cdot Sepal.Length + \beta_{22}\cdot Sepal.Width + \beta_{23}\cdot Petal.Length + \beta_{24}\cdot Petal.Width X_{i}$$
then we get:

when it is useful and/or why it is important
important information about the topic(model theory)
sources for the information you provide and additional resources for learning more about the topic

the scope of your tutorial

reasons, if any, you could not obtain the same results from all three language examples.

# {.tabset}

## R
### Load the data
First, we need to load iris data into R. It is possible to download the data from [here](https://www.kaggle.com/vvenanccio/irisflowers), but iris dataset is already contained in R. Therefore, just by loading it, iris data will be available and can be used.

```{r}
data(iris)
```

### Description of the data 
This is what the first 6 rows look like. 

```{r}
head(iris)
```

We can get basic descriptives for the entire data set by using summary.

```{r}
summary(iris)
```

### Seperate the data into training set and testing set

We partition the data into two parts - a training set consisting of the first 40 observations for each species and a testing set of the remaining 10 observations for each species. We use a training set to build and estimate the model and a testing set to see how well the model does on new data not used in the constuction of the model.

```{r}
train_1 = iris[1:40,]; train_2 = iris[51:90,]; train_3 = iris[101:140,]
train = rbind(train_1,train_2); train = rbind(train, train_3)
test_1 = iris[41:50,]; test_2 = iris[91:100,]; test_3 = iris[141:150,]
test = rbind(test_1,test_2); test = rbind(test, test_3)
```

### Build the multinomial logistic regression model using the train set
Below we use the *multinorm* function from the nnet package to estimate a multinomial logistic regression model. Before running our model, we choose the level of our outcome that we wish to use our baseline and specify this in the *relevel* function. We pick "virginica" as our baseline. Then, we run our model using *multinom*. The nnet package does not include p-value calculation for the regression coefficients, so we calculate p-values using Wald tests(here z-tests).  

```{r}
library(nnet)
train$species2 = relevel(train$Species, ref = "virginica")
model = multinom(species2 ~ Sepal.Length + Sepal.Width + 
                    Petal.Length + Petal.Width , data = train)
summary(model)
z = summary(model)$coefficients/summary(model)$standard.errors
z
p = (1 - pnorm(abs(z), 0, 1)) * 2
p
```

we first see that some output is generated by running the model, even though we are assigning the model to a new R object. This model-running output includes some iteration history and include the final negative log-likelihood **5.923148**. This value multiplied by two is then see in the model summary as the Residual Deviance.

The model summary output has a block of coefficients and a block of standard errors. Each of these blocks has one row of values corresponding to a model equation. Two models are tested in this multinomial regression, one comparing membership to setosa versus virginica and one comparing membership to versicolor versus virginica. They correspond to the two equations below:

\[ \begin{split} ln\frac{Pr(Species = setosa)}{Pr(Species = virginica)} = & 5.28918 +10.978504\times Sepal.Length + 16.793690\times Sepal.Width\\ & -23.702171 \times Petal.Length -18.17830\times Petal.Width \end{split} \] \[ \begin{split} ln\frac{Pr(Species = versicolor)}{Pr(Species = virginica)} = & 41.55886 + 2.416804\times Sepal.Length +  6.591888\times Sepal.Width\\ & -9.215024\times Petal.Length -17.93605\times Petal.Width \end{split} \]

Through the p-values, we can see all of the coefficients of the first model(setosa vs. virginica) is not significant while some of the coefficients of the second model(versicolor vs. virginica) is significant. It means we may need to combine setosa and virginica. Then the model becomes a binomial logistic regression model.  


### Test the accuracy of model using the test set

Now we can test our model using a test set data. We compute the accuracy rate. The result is 1. Good performance!


```{r}
library(data.table)
a = predict(model, newdata =test, "probs")
a = data.table(a)
b = rep(1,30)                
b[which(a$versicolor == apply(a, 1, max))]=2              
b[which(a$virginica == apply(a, 1, max))]=3     
c = c(rep(1,10), rep(2,10), rep(3,10))
accuracy = sum(b==c)/30
accuracy
```


### References
See [UCLA Institute for Digital Research and Education](https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/) for another example of multinomial logistic regression and other statistical techniques in mutliple programming languages.

## SAS

### Read the data into SAS
We use proc import to import the dataset.
```sas
FILENAME REFFILE '/Iris.csv';

PROC IMPORT DATAFILE=REFFILE
	DBMS=CSV
	OUT=work.import;
	GETNAMES=YES;
RUN;
```

### Description of Data
We get basic descriptives for the entire data set by using proc summary.
```sas
proc summary data=work.import min q1 mean q3 max std n;
var Sepal_Length Sepal_Width Petal_Length Petal_Width;
by Species;
output out = iris_summary;

proc print data = iris_summary;
title 'Summary of Iris data set';
run;
```
<div align=center> 
![](https://github.com/SUIIAO/STATS506_GROUP12/raw/master/SAS/sas_data_summary.PNG)

### Separate the data into train set and test set
```sas
data train;
set work.import;
if _N_ in (1:40, 51:90, 101:140) then output;
run;

data test;
set work.import;
if _N_ in (41:50, 91:100, 141:150) then output;
run;
```

### Build the multinomial logistic regression model by train set
Below we use proc logistic to estimate a multinomial logistic regression model. We can specify the baseline category for species using (ref = “virginic”). Then we get the detailed fitting result. Finally, we use test set to predict their species.

```sas
proc logistic data = train;
class species (ref = "virginic");
model species =  Sepal_Length Sepal_Width Petal_Length Petal_Width / link = glogit;
score data=test out=valpred;
title 'Multinomial Logistic Regression Model';
run;
```
<div align=center> 
![](https://github.com/SUIIAO/STATS506_GROUP12/raw/master/SAS/model_summary_1.PNG)
![](https://github.com/SUIIAO/STATS506_GROUP12/raw/master/SAS/model_summary_2.PNG)

<div align=left> 
In the output above, the likelihood ratio chi-square of 251.8012 with a p-value < 0.0001 tells us that our model as a whole fits significantly better than an empty model (i.e., a model with no predictors)

Two models are tested in this multinomial regression, one comparing membership to setosa versus virginic and one comparing membership to versicolor versus virginic. They correspond to the two equations below:

$$
\begin{split}
ln\frac{Pr(Species = setosa)}{Pr(Species = virginica)} = & 35.8688 +2.3092\times Sepal.Length + 13.1311\times Sepal.Width\\ &- 12.5865\times Petal.Length - 23.8865\times Petal.Width
\end{split}
$$
$$
\begin{split}
ln\frac{Pr(Species = versicolor)}{Pr(Species = virginica)} = & 41.1253 +2.4109\times Sepal.Length + 6.5280\times Sepal.Width\\ &- 9.1397\times Petal.Length - 17.7683\times Petal.Width
\end{split}
$$

<div align=left>
### Test the accuracy of model by test set
Based on the fitted model, now we can predict the each species probability of the testing data set. Each predicted species probability is between 0 and 1, hence we regard the highest probability one as its species. 

Compared to the true species situation, we compute the accuracy rate which is 1. Good performance!
```sas
data prediction;
 set valpred;
 species_pre = 0;
 species_ori = 0;
 accuracy = 1;
 if species = 'setosa' then species_ori=1;
 if species = 'versicol' then species_ori=2; 
 if species = 'virginic' then species_ori=3;
 if P_setosa > P_versicol and p_setosa > P_virginic then species_pre=1;
 if P_versicol > P_setosa and P_versicol > P_virginic then species_pre=2; 
 if P_virginic > P_setosa and P_virginic > P_versicol then species_pre=3;
 if species_ori > species_pre then delete;
 if species_ori < species_pre then delete;
 keep species accuracy;

proc summary data=prediction;
output out=accuracy
sum(accuracy) = accuracy;

data final;
set accuracy;
accuracy = accuracy/30;

proc print data=final;
title 'Test the Accuracy by Test Set';
run;
```
<div align=center> 
![](https://github.com/SUIIAO/STATS506_GROUP12/raw/master/SAS/Accuracy.PNG)



### References
- See [UCLA Institute for Digital Research and Education](https://stats.idre.ucla.edu/sas/dae/multinomiallogistic-regression/) for another example of multinomial logistic regression and other statistical techniques in mutliple programming languages.

## Stata
